{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.nn import Module, Sequential, Linear, ReLU\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 10000\n",
      "val: 1000\n",
      "test: 1000\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import ZINC\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "root = \"./data/zinc\"\n",
    "\n",
    "# mean and std of training set\n",
    "MEAN = torch.Tensor([0.0153])\n",
    "STD = torch.Tensor([2.0109])\n",
    "\n",
    "def transform(data):\n",
    "    data.x = F.one_hot(data.x, 28).squeeze(1).float()\n",
    "    data.y = (data.y - MEAN) / STD\n",
    "    data.edge_attr = F.one_hot(data.edge_attr - 1, 3).squeeze(1).float()\n",
    "    return data\n",
    "\n",
    "# train_dataset = ZINC(root, split=\"train\", transform=transform)  # subset=False\n",
    "# val_dataset = ZINC(root, split='val', transform=transform)\n",
    "# test_dataset = ZINC(root, split=\"test\", transform=transform)\n",
    "\n",
    "train_dataset = ZINC(root, subset=True, split=\"train\", transform=transform)\n",
    "val_dataset = ZINC(root, subset=True, split='val', transform=transform)\n",
    "test_dataset = ZINC(root, subset=True, split=\"test\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"train: {len(train_dataset)}\")\n",
    "print(f\"val: {len(val_dataset)}\")\n",
    "print(f\"test: {len(test_dataset)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "def experiment(\n",
    "        model,\n",
    "        train_loader, val_loader,\n",
    "        test_loader=None,\n",
    "        num_epoch=100,\n",
    "        ):\n",
    "\n",
    "    MEAN = torch.Tensor([0.0153]).to(device)\n",
    "    STD = torch.Tensor([2.0109]).to(device)\n",
    "    def unwhiten(y):\n",
    "        return y * STD + MEAN\n",
    "\n",
    "    # regression loss\n",
    "    loss_fn = nn.MSELoss()\n",
    "    eval_fn = nn.L1Loss()\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "\n",
    "        loss_sum = 0\n",
    "        score_sum = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(data)\n",
    "            loss = loss_fn(y_pred, data.y[:, None])  # mean loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "            optimizer.step()\n",
    "\n",
    "            score = eval_fn(unwhiten(y_pred), unwhiten(data.y[:, None]))\n",
    "            loss_sum += loss.item() * data.num_graphs\n",
    "            score_sum += score.item() * data.num_graphs\n",
    "\n",
    "        train_loss = loss_sum / len(train_loader.dataset)\n",
    "        train_score = score_sum / len(train_loader.dataset)\n",
    "        return train_loss, train_score\n",
    "    \n",
    "    def eval(loader):  # for validation / test\n",
    "        model.eval()\n",
    "        loss = 0\n",
    "        score = 0\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(data)\n",
    "                loss += loss_fn(y_pred, data.y[:, None]).item() * data.num_graphs\n",
    "                score += eval_fn(unwhiten(y_pred), unwhiten(data.y[:, None])).item() * data.num_graphs\n",
    "        loss = loss / len(loader.dataset)\n",
    "        score = score / len(loader.dataset)\n",
    "        return loss, score\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = ExponentialLR(optimizer, 0.95)\n",
    "\n",
    "    train_loss_hist = []\n",
    "    train_score_hist = []\n",
    "    val_loss_hist = []\n",
    "    val_score_hist = []\n",
    "    \n",
    "    val_loss_best = np.inf\n",
    "    epoch_best = None\n",
    "\n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "        \n",
    "        # train\n",
    "        train_loss, train_score = train()\n",
    "        train_loss_hist.append(train_loss)\n",
    "        train_score_hist.append(train_score)\n",
    "\n",
    "        # validation\n",
    "        val_loss, val_score = eval(val_loader)\n",
    "        val_loss_hist.append(val_loss)\n",
    "        val_score_hist.append(val_score)\n",
    "\n",
    "        if val_loss < val_loss_best:\n",
    "            val_loss_best = val_loss\n",
    "            val_score_best = val_score\n",
    "            epoch_best = epoch\n",
    "\n",
    "            if test_loader is not None:\n",
    "                test_loss, test_score = eval(test_loader)\n",
    "\n",
    "        if epoch == 1 or epoch % 1 == 0:\n",
    "            output = f\"Epoch {epoch}, train loss {train_loss:.4f}, val loss {val_loss:.4f}; Best Epoch {epoch_best}, val score = {val_score_best:.4f}\"\n",
    "            if test_loader is not None:\n",
    "                output = output + f\", test score = {test_score:.4f}\"\n",
    "            print(output)\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss_hist\": train_loss_hist,\n",
    "        \"train_score_hist\": train_score_hist,\n",
    "        \"val_loss_hist\": val_loss_hist,\n",
    "        \"val_score_hist\": val_score_hist,\n",
    "        \"best_epoch\": epoch_best,\n",
    "        \"val_loss\": val_loss_best,\n",
    "        \"val_score\": val_score_best,\n",
    "    }\n",
    "    if test_loader is not None:\n",
    "        ret[\"test_score\"] = test_score\n",
    "    return ret"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression_Head(Module):\n",
    "    def __init__(self, in_dim, out_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = Sequential(\n",
    "            Linear(in_dim, hid_dim),\n",
    "            ReLU(),\n",
    "            Linear(hid_dim, out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DE-MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DE_Layer(MessagePassing):\n",
    "    def __init__(self, emb_dim, num_iter):\n",
    "        super().__init__(aggr='add')\n",
    "        self.num_iter = num_iter\n",
    "        \n",
    "        self.mlp_update = Sequential(\n",
    "            Linear(2 * emb_dim, emb_dim),\n",
    "            ReLU(),\n",
    "            Linear(emb_dim, emb_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, h, edge_index, fixed_x=None):\n",
    "        if fixed_x is not None:\n",
    "            x = fixed_x\n",
    "        else:\n",
    "            x = h\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "            h = self.propagate(edge_index, h=h, x=x)\n",
    "\n",
    "        return h\n",
    "    \n",
    "    def message(self, h_j):\n",
    "        return h_j\n",
    "    \n",
    "    def update(self, aggr_out, x):\n",
    "        return self.mlp_update(torch.cat([x, aggr_out], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DE_Model(Module):\n",
    "    def __init__(self, in_dim, out_dim, emb_dim=64, hid_dim=64, num_iter=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_in = Linear(in_dim, emb_dim)\n",
    "        self.emb = DE_Layer(emb_dim, num_iter)\n",
    "        self.pool = global_mean_pool\n",
    "        self.head = Regression_Head(emb_dim, out_dim, hid_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        h = self.lin_in(data.x)\n",
    "        h = self.emb(h, data.edge_index)\n",
    "        h = self.pool(h, data.batch)\n",
    "        h = self.head(h)\n",
    "        return h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DE-MF + edge feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DE_Layer(MessagePassing):\n",
    "    def __init__(self, emb_dim, num_iter):\n",
    "        super().__init__(aggr='add')\n",
    "        self.num_iter = num_iter\n",
    "        \n",
    "        self.mlp_update = Sequential(\n",
    "            Linear(2 * emb_dim, emb_dim),\n",
    "            ReLU(),\n",
    "            Linear(emb_dim, emb_dim),\n",
    "        )\n",
    "\n",
    "        self.mlp_message = Sequential(\n",
    "            Linear(emb_dim + 3, emb_dim),\n",
    "            ReLU(),\n",
    "            Linear(emb_dim, emb_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, h, edge_index, edge_attr, fixed_x=None):\n",
    "        if fixed_x is not None:\n",
    "            x = fixed_x\n",
    "        else:\n",
    "            x = h\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "            h = self.propagate(edge_index, h=h, x=x, edge_attr=edge_attr)\n",
    "\n",
    "        return h\n",
    "    \n",
    "    def message(self, h_j, edge_attr):\n",
    "        return self.mlp_message(torch.cat([h_j, edge_attr], dim=-1))\n",
    "    \n",
    "    def update(self, aggr_out, x):\n",
    "        return self.mlp_update(torch.cat([x, aggr_out], dim=-1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP_Layer(MessagePassing):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__(aggr='add')\n",
    "        \n",
    "        self.mlp_update = Sequential(\n",
    "            Linear(2 * emb_dim, emb_dim),\n",
    "            ReLU(),\n",
    "            Linear(emb_dim, emb_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, h, edge_index, fixed_x=None):\n",
    "        if fixed_x is not None:\n",
    "            x = fixed_x\n",
    "        else:\n",
    "            x = h\n",
    "            \n",
    "        h = self.propagate(edge_index, h=h, x=x)\n",
    "\n",
    "        return h\n",
    "    \n",
    "    def message(self, h_j):\n",
    "        return h_j\n",
    "    \n",
    "    def update(self, aggr_out, x):\n",
    "        return self.mlp_update(torch.cat([x, aggr_out], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP_Model(Module):\n",
    "    def __init__(self, in_dim, out_dim, num_layer=4, emb_dim=256, hid_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_in = Linear(in_dim, emb_dim)\n",
    "        \n",
    "        self.embs = nn.ModuleList()\n",
    "        for layer in range(num_layer):\n",
    "            self.embs.append(MP_Layer(emb_dim))\n",
    "\n",
    "        self.pool = global_mean_pool\n",
    "        self.head = Regression_Head(emb_dim, out_dim, hid_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        h = self.lin_in(data.x)\n",
    "        for emb in self.embs:\n",
    "            h = emb(h, data.edge_index)\n",
    "        h = self.pool(h, data.batch)\n",
    "        h = self.head(h)\n",
    "        return h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP_reuse(Module):\n",
    "    def __init__(self, in_dim, out_dim, num_layer=4, emb_dim=256, hid_dim=256):\n",
    "        super().__init__()\n",
    "        self.num_layer = num_layer\n",
    "\n",
    "        self.lin_in = Linear(in_dim, emb_dim)\n",
    "        self.emb = MP_Layer(emb_dim)\n",
    "        self.pool = global_mean_pool\n",
    "        self.head = Regression_Head(emb_dim, out_dim, hid_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        h = self.lin_in(data.x)\n",
    "        for layer in range(self.num_layer):\n",
    "            h = self.emb(h, data.edge_index)\n",
    "        h = self.pool(h, data.batch)\n",
    "        h = self.head(h)\n",
    "        return h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP_FX(Module):\n",
    "    def __init__(self, in_dim, out_dim, num_layer=4, emb_dim=256, hid_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_in = Linear(in_dim, emb_dim)\n",
    "        \n",
    "        self.embs = nn.ModuleList()\n",
    "        for layer in range(num_layer):\n",
    "            self.embs.append(MP_Layer(emb_dim))\n",
    "\n",
    "        self.pool = global_mean_pool\n",
    "        self.head = Regression_Head(emb_dim, out_dim, hid_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        h = self.lin_in(data.x)\n",
    "\n",
    "        fixed_x = h\n",
    "        for emb in self.embs:\n",
    "            h = emb(h, data.edge_index, fixed_x=fixed_x)\n",
    "\n",
    "        h = self.pool(h, data.batch)\n",
    "        h = self.head(h)\n",
    "        return h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked DE-MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DE_Model(Module):\n",
    "    def __init__(self, in_dim, out_dim, num_iter=4, emb_dim=256, hid_dim=256, num_layer=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_in = Linear(in_dim, emb_dim)\n",
    "\n",
    "        self.embs = nn.ModuleList()\n",
    "        for layer in range(num_layer):\n",
    "            self.embs.append(DE_Layer(emb_dim, num_iter))\n",
    "\n",
    "        self.pool = global_mean_pool\n",
    "        self.head = Regression_Head(emb_dim, out_dim, hid_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        h = self.lin_in(data.x)\n",
    "        for emb in self.embs:\n",
    "            h = emb(h, data.edge_index)\n",
    "        h = self.pool(h, data.batch)\n",
    "        h = self.head(h)\n",
    "        return h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterated MPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP_Model(Module):\n",
    "    def __init__(self, in_dim, out_dim, num_layer=4, emb_dim=256, hid_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_in = Linear(in_dim, emb_dim)\n",
    "        \n",
    "        self.embs = nn.ModuleList()\n",
    "        for layer in range(num_layer):\n",
    "            self.embs.append(MP_Layer(emb_dim))\n",
    "\n",
    "        self.pool = global_mean_pool\n",
    "        self.head = Regression_Head(emb_dim, out_dim, hid_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        h = self.lin_in(data.x)\n",
    "        for emb in self.embs:\n",
    "            h = emb(h, data.edge_index)\n",
    "        h = self.pool(h, data.batch)\n",
    "        h = self.head(h)\n",
    "        return h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
